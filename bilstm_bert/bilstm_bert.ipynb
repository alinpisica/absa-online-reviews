{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.legacy import data\n",
    "\n",
    "from BiLSTM import BiLSTM\n",
    "\n",
    "import random\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_PATH = './bert_fine_tuned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2060 SUPER\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM with BERT as embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "N_EPOCHS = 15\n",
    "\n",
    "train_data_path = './data/semeval_train_data.csv'\n",
    "output_path_result = './results/' + 'bilstm_bert_train.csv'\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\torchtext\\data\\utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(\n",
    "    tokenize = 'spacy',\n",
    "    batch_first = True,\n",
    "    include_lengths = True)\n",
    "\n",
    "LABEL = data.LabelField(\n",
    "    dtype = torch.long,\n",
    "    batch_first = True,\n",
    "    use_vocab = False)\n",
    "\n",
    "fields = [('text',TEXT),('label', LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Judging', 'from', 'previous', 'posts', 'this', 'used', 'to', 'be', 'a', 'good', 'place', ';', 'but', 'not', 'any', 'longer', '.'], 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "training_data = data.TabularDataset(\n",
    "    path = train_data_path,\n",
    "    format = 'csv',\n",
    "    fields = fields,\n",
    "    skip_header = True)\n",
    "\n",
    "print(vars(training_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = training_data.split(\n",
    "    split_ratio = 0.8, \n",
    "    random_state = random.seed(SEED)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 3468\n",
      "Size of LABEL vocabulary: 3\n",
      "[('.', 1309), ('the', 943), (';', 826), ('and', 678), ('I', 479), ('a', 458), ('is', 421), ('to', 416), ('was', 367), ('of', 286)]\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(\n",
    "    train_data, \n",
    "    vectors='glove.6B.300d'\n",
    ")\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "#No. of unique tokens in text\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "#No. of unique tokens in label\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "#Commonly used words\n",
    "print(TEXT.vocab.freqs.most_common(10))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, do_lower_case=True)\n",
    "\n",
    "bert_model = BertForSequenceClassification.from_pretrained(BERT_MODEL_PATH, num_labels=3)\n",
    "bert_model.to(device)\n",
    "\n",
    "config = BertConfig.from_pretrained(BERT_MODEL_PATH)\n",
    "\n",
    "optimus = BertForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30522, 768])\n"
     ]
    }
   ],
   "source": [
    "# Uncomment for printing the params\n",
    "# dict(model.named_parameters())\n",
    "\n",
    "word_emb_weight_param = 'bert.embeddings.word_embeddings.weight'\n",
    "\n",
    "word_emb_w = dict(bert_model.named_parameters())[word_emb_weight_param]\n",
    "\n",
    "print(word_emb_w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_vocab, embedding_dim = word_emb_w.shape\n",
    "\n",
    "num_hidden_nodes = 32\n",
    "num_output_nodes = 3\n",
    "num_layers = 2\n",
    "bidirection = True\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(\n",
    "    size_of_vocab, \n",
    "    embedding_dim, \n",
    "    num_hidden_nodes, \n",
    "    num_output_nodes, \n",
    "    num_layers, \n",
    "    bidirectional = bidirection, \n",
    "    dropout = dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM(\n",
      "  (embedding): Embedding(30522, 768)\n",
      "  (lstm): LSTM(768, 32, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30522, 768])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = word_emb_w\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.MultiMarginLoss()\n",
    "\n",
    "#define metric\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "    \n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "    \n",
    "#push to cuda if available\n",
    "bilstm_bert = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bilstm_bert(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    #initialize every epoch \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1 = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "    \n",
    "    #set the model in training phase\n",
    "    model.train()  \n",
    "    \n",
    "    for batch in iterator:\n",
    "        #resets the gradients after every batch\n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        #retrieve text and no. of words\n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        predictions = model(text, text_lengths.cpu()).cpu()\n",
    "        predicted = torch.argmax(predictions, dim = 1)\n",
    "\n",
    "        labels = batch.label.cpu()\n",
    "        #compute the loss\n",
    "        loss = criterion(predictions, labels)        \n",
    "        \n",
    "        #compute the binary accuracy\n",
    "        # acc = binary_accuracy(predictions, batch.label)   \n",
    "        acc = (predicted == labels).sum()\n",
    "\n",
    "        #round predictions to the closest integer\n",
    "        # predicted = torch.round(predictions).tolist()\n",
    "        real = labels.tolist()\n",
    "\n",
    "        #compute the f1_score\n",
    "        f1score = f1_score(real, predicted, average=\"macro\", zero_division=0)\n",
    "\n",
    "        #compute the precision\n",
    "        precision = precision_score(real, predicted, average=\"macro\", zero_division=0)\n",
    "\n",
    "        #compute the recall\n",
    "        recall = recall_score(real, predicted, average=\"macro\", zero_division=0)\n",
    "        \n",
    "        #backpropage the loss and compute the gradients\n",
    "        loss.backward()       \n",
    "        \n",
    "        #update the weights\n",
    "        optimizer.step()      \n",
    "        \n",
    "        #loss and accuracy\n",
    "        epoch_loss += loss.item() / len(batch)\n",
    "        epoch_acc += acc.item() / len(batch)\n",
    "        epoch_f1 += f1score\n",
    "        epoch_precision += precision\n",
    "        epoch_recall += recall \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1 / len(iterator), epoch_precision / len(iterator), epoch_recall / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bilstm_bert(model, iterator, criterion):\n",
    "    #initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1 = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "\n",
    "    #deactivating dropout layers\n",
    "    model.eval()\n",
    "    predicted = []\n",
    "    #deactivates autograd\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            #retrieve text and no. of words\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            labels = batch.label.cpu()\n",
    "            predictions = model(text, text_lengths.cpu()).cpu()\n",
    "            predicted = torch.argmax(predictions, dim = 1)\n",
    "            \n",
    "            #compute loss and accuracy\n",
    "            loss = criterion(predictions, labels)\n",
    "            # acc = binary_accuracy(predictions, batch.label)\n",
    "            acc = (predicted == labels).sum()\n",
    "\n",
    "            #round predictions to the closest integer\n",
    "            real = labels.tolist()\n",
    "\n",
    "            #compute the f1_score\n",
    "            f1score = f1_score(real, predicted, average=\"macro\", zero_division=0)\n",
    "\n",
    "            #compute the precision\n",
    "            precision = precision_score(real, predicted, average=\"macro\", zero_division=0)\n",
    "\n",
    "            #compute the recall\n",
    "            recall = recall_score(real, predicted, average=\"macro\", zero_division=0)\n",
    "            \n",
    "            #keep track of loss and accuracy\n",
    "            epoch_loss += loss.item() / len(batch)\n",
    "            epoch_acc += acc.item() / len(batch)\n",
    "            epoch_f1 += f1score\n",
    "            epoch_precision += precision\n",
    "            epoch_recall += recall\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1 / len(iterator), epoch_precision / len(iterator), epoch_recall / len(iterator), predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printProgress(epoch, total_epochs, train_loss, validation_loss, train_acc, validation_acc):\n",
    "    print(f\"Iteration {epoch} / {total_epochs}\")\n",
    "    print(f\"\\tTrain loss: {train_loss*100:.2f}\")\n",
    "    print(f\"\\tValidation loss: {validation_loss*100:.2f}\")\n",
    "    print(f\"\\tTrain accuracy: {train_acc*100:.2f}\")\n",
    "    print(f\"\\tValidation accuracy: {validation_acc*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5 / 15\n",
      "\tTrain loss: 0.18\n",
      "\tValidation loss: 0.97\n",
      "\tTrain accuracy: 88.31\n",
      "\tValidation accuracy: 60.71\n",
      "Iteration 10 / 15\n",
      "\tTrain loss: 0.01\n",
      "\tValidation loss: 1.22\n",
      "\tTrain accuracy: 99.75\n",
      "\tValidation accuracy: 62.50\n",
      "Iteration 15 / 15\n",
      "\tTrain loss: 0.00\n",
      "\tValidation loss: 1.25\n",
      "\tTrain accuracy: 99.88\n",
      "\tValidation accuracy: 61.83\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "columns = 'Name,Epoch,Loss,Acc,F1,Precision,Recall,Predicted'\n",
    "f = open(output_path_result, 'w')\n",
    "f.write(columns)\n",
    "f.write('\\n')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    #train the model\n",
    "    train_loss, train_acc, train_f1, train_precision, train_recall = train_bilstm_bert(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    #evaluate the model\n",
    "    validation_loss, validation_acc, validation_f1, validation_precision, validation_recall, predicted = evaluate_bilstm_bert(model, valid_iterator, criterion)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        printProgress(epoch + 1, N_EPOCHS, train_loss, validation_loss, train_acc, validation_acc)\n",
    "\n",
    "    #save the best model\n",
    "    if validation_loss < best_valid_loss:\n",
    "        best_valid_loss = validation_loss\n",
    "        torch.save(model.state_dict(), './bilstm_bert_model/saved_weights.pt')\n",
    "\n",
    "    f.write(f'BERT_BILSTM_Train,{epoch},{train_loss*100:.2f},{train_acc*100:.2f},{train_f1*100:.2f},{train_precision*100:.2f},{train_recall*100:.2f},{predicted}')\n",
    "    f.write('\\n')\n",
    "    f.write(f'BERT_BILSTM_Test,{epoch},{validation_loss*100:.2f},{validation_acc*100:.2f},{validation_f1*100:.2f},{validation_precision*100:.2f},{validation_recall*100:.2f},{predicted}')\n",
    "    f.write('\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './bilstm_bert_model/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model on the Gold dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Judging', 'from', 'previous', 'posts', 'this', 'used', 'to', 'be', 'a', 'good', 'place', ';', 'but', 'not', 'any', 'longer', '.'], 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "test_data_path = './data/semeval_train_data.csv'\n",
    "\n",
    "fields = [('text',TEXT),('label', LABEL)]\n",
    "\n",
    "test_data = data.TabularDataset(\n",
    "    path = test_data_path,\n",
    "    format = 'csv',\n",
    "    fields = fields,\n",
    "    skip_header = True\n",
    ")\n",
    "\n",
    "print(vars(test_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 3946\n",
      "Size of LABEL vocabulary: 3\n",
      "[('.', 1629), ('the', 1165), (';', 1032), ('and', 860), ('I', 599), ('a', 577), ('is', 524), ('to', 514), ('was', 464), ('of', 352)]\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(\n",
    "    test_data, \n",
    "    vectors='glove.6B.300d'\n",
    ")\n",
    "LABEL.build_vocab(test_data)\n",
    "\n",
    "#No. of unique tokens in text\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "#No. of unique tokens in label\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "#Commonly used words\n",
    "print(TEXT.vocab.freqs.most_common(10))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "test_iterator = data.BucketIterator(\n",
    "    test_data, \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.50\n",
      "Test accuracy: 40.53\n",
      "Test F1: 33.52\n",
      "Test precision: 34.68\n",
      "Test recall: 34.93\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy, f1, precision, recall, predicted = evaluate_bilstm_bert(model, test_iterator, criterion)\n",
    "\n",
    "print(f\"Test loss: {loss*100:.2f}\")\n",
    "print(f\"Test accuracy: {accuracy*100:.2f}\")\n",
    "print(f\"Test F1: {f1*100:.2f}\")\n",
    "print(f\"Test precision: {precision*100:.2f}\")\n",
    "print(f\"Test recall: {recall*100:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
