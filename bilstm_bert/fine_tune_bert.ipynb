{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import random\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2060 SUPER\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in dataset: 2000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>Good luck getting a table.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>The food was actually aweful.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Admittedly some nights inside the restaurant w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>Very good wine choices.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>less wait time for me!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Review  Polarity\n",
       "1191                         Good luck getting a table.         2\n",
       "1157                      The food was actually aweful.         0\n",
       "177   Admittedly some nights inside the restaurant w...         0\n",
       "571                             Very good wine choices.         1\n",
       "821                              less wait time for me!         2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./data/semeval_train_data.csv', encoding='ISO-8859-1')\n",
    "\n",
    "print(f\"Number of entries in dataset: {train_df.shape[0]}\")\n",
    "\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(text):\n",
    "    # Remove non-ASCII chars\n",
    "    re_nonascii = r'[^\\x00-\\x7F]+'\n",
    "    text = re.sub(re_nonascii, '', text)\n",
    "\n",
    "    # Remove URLs\n",
    "    re_url = r'https?://[A-Za-z0-9./]+'\n",
    "    text = re.sub(re_url, '', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    re_puncuation = r'[\\.\\,\\!\\?\\:\\;\\-\\=\\(\\)\\[\\]\\\"\\'\\%\\*\\#\\@]'\n",
    "    text = re.sub(re_puncuation, \" \", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>The exotic food is beautifully presented and i...</td>\n",
       "      <td>1</td>\n",
       "      <td>The exotic food is beautifully presented and i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>A guaranteeed delight!</td>\n",
       "      <td>1</td>\n",
       "      <td>A guaranteeed delight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>I went here for a family birthday dinner.</td>\n",
       "      <td>2</td>\n",
       "      <td>I went here for a family birthday dinner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>Their bagels are fine; but they are a little o...</td>\n",
       "      <td>0</td>\n",
       "      <td>Their bagels are fine  but they are a little o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>I asked the chef what he recommends and he ask...</td>\n",
       "      <td>2</td>\n",
       "      <td>I asked the chef what he recommends and he ask...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>I will NEVER return.</td>\n",
       "      <td>0</td>\n",
       "      <td>I will NEVER return</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>The hostess is rude to the point of being offe...</td>\n",
       "      <td>0</td>\n",
       "      <td>The hostess is rude to the point of being offe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>Don't miss Bloom's on your next trip to Manhat...</td>\n",
       "      <td>1</td>\n",
       "      <td>Don t miss Bloom s on your next trip to Manhat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557</th>\n",
       "      <td>A fairly late entry into the haute barnyard sw...</td>\n",
       "      <td>2</td>\n",
       "      <td>A fairly late entry into the haute barnyard sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>Too bad I had paid an extra $2 for the stone b...</td>\n",
       "      <td>0</td>\n",
       "      <td>Too bad I had paid an extra $2 for the stone b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Review  Polarity  \\\n",
       "202   The exotic food is beautifully presented and i...         1   \n",
       "558                              A guaranteeed delight!         1   \n",
       "1704         I went here for a family birthday dinner.          2   \n",
       "1032  Their bagels are fine; but they are a little o...         0   \n",
       "249   I asked the chef what he recommends and he ask...         2   \n",
       "1596                               I will NEVER return.         0   \n",
       "127   The hostess is rude to the point of being offe...         0   \n",
       "1134  Don't miss Bloom's on your next trip to Manhat...         1   \n",
       "1557  A fairly late entry into the haute barnyard sw...         2   \n",
       "1768  Too bad I had paid an extra $2 for the stone b...         0   \n",
       "\n",
       "                                           cleaned_text  \n",
       "202   The exotic food is beautifully presented and i...  \n",
       "558                              A guaranteeed delight   \n",
       "1704         I went here for a family birthday dinner    \n",
       "1032  Their bagels are fine  but they are a little o...  \n",
       "249   I asked the chef what he recommends and he ask...  \n",
       "1596                               I will NEVER return   \n",
       "127   The hostess is rude to the point of being offe...  \n",
       "1134  Don t miss Bloom s on your next trip to Manhat...  \n",
       "1557  A fairly late entry into the haute barnyard sw...  \n",
       "1768  Too bad I had paid an extra $2 for the stone b...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['cleaned_text'] = list(map(lambda x: clean_review(x), train_df['Review']))\n",
    "\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = train_df['cleaned_text'].values\n",
    "labels = train_df['Polarity'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original review:\n",
      "\t Judging from previous posts this used to be a good place  but not any longer \n",
      "Tokenized review\n",
      "\t ['judging', 'from', 'previous', 'posts', 'this', 'used', 'to', 'be', 'a', 'good', 'place', 'but', 'not', 'any', 'longer']\n",
      "Token IDs for review\n",
      "\t [13325, 2013, 3025, 8466, 2023, 2109, 2000, 2022, 1037, 2204, 2173, 2021, 2025, 2151, 2936]\n"
     ]
    }
   ],
   "source": [
    "# Testing for a simple input\n",
    "\n",
    "review_sample = reviews[0]\n",
    "\n",
    "print('Original review:')\n",
    "print('\\t', review_sample)\n",
    "\n",
    "print('Tokenized review')\n",
    "print('\\t', tokenizer.tokenize(review_sample))\n",
    "\n",
    "print('Token IDs for review')\n",
    "print('\\t', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(review_sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max review length:  71\n",
      "Max review length after rounding up:  80\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "for review in reviews:\n",
    "    input_ids = tokenizer.encode(review, add_special_tokens=True)\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max review length: ', max_len)\n",
    "\n",
    "# Will round max_len to the nearest multiple of 10 just to be sure\n",
    "max_len = max_len + int(round(max_len, -1) - max_len) % 10\n",
    "\n",
    "print('Max review length after rounding up: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  We  there were four of us  arrived at noon   the place was empty   and the staff acted like we were imposing on them and they were very rude \n",
      "Token IDs: tensor([  101,  2057,  2045,  2020,  2176,  1997,  2149,  3369,  2012, 11501,\n",
      "         1996,  2173,  2001,  4064,  1998,  1996,  3095,  6051,  2066,  2057,\n",
      "         2020, 16625,  2006,  2068,  1998,  2027,  2020,  2200, 12726,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for review in reviews:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        review,                    # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_len,           # Pad & truncate all sentences.\n",
    "                        padding='max_length',\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                        truncation = True,\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', reviews[1])\n",
    "print('Token IDs:', input_ids[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split - 80% training, 20% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_pct = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples count: 1600\n",
      "Validation samples count: 400\n"
     ]
    }
   ],
   "source": [
    "train_size = int(training_data_pct * len(dataset))\n",
    "validation_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, validation_dataset = random_split(dataset, [train_size, validation_size])\n",
    "\n",
    "print(f\"Training samples count: {train_size}\")\n",
    "print(f\"Validation samples count: {validation_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size of positive reviews: 860\n",
      "Validation size of positive reviews: 204\n",
      "Positive samples ratio train to validation: 4.215686274509804\n",
      "\n",
      "Training size of negative reviews: 441\n",
      "Validation size of negative reviews: 120\n",
      "Negative samples ratio train to validation: 4.215686274509804\n",
      "\n",
      "Training size of neutral reviews: 860\n",
      "Validation size of neutral reviews: 204\n",
      "Neutral samples ratio train to validation: 4.215686274509804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check distribution of samples\n",
    "\n",
    "t_pos_cnt = 0\n",
    "t_neg_cnt = 0\n",
    "t_neutr_cnt = 0\n",
    "\n",
    "for _, _, lbl in train_dataset:\n",
    "    if lbl == 1:\n",
    "        t_pos_cnt += 1\n",
    "    if lbl == 0:\n",
    "        t_neg_cnt += 1\n",
    "    if lbl == 1:\n",
    "        t_neutr_cnt += 1\n",
    "\n",
    "v_pos_cnt = 0\n",
    "v_neg_cnt = 0\n",
    "v_neutr_cnt = 0\n",
    "\n",
    "for _, _, lbl in validation_dataset:\n",
    "    if lbl == 1:\n",
    "        v_pos_cnt += 1\n",
    "    if lbl == 0:\n",
    "        v_neg_cnt += 1\n",
    "    if lbl == 1:\n",
    "        v_neutr_cnt += 1\n",
    "\n",
    "print(f\"Training size of positive reviews: {t_pos_cnt}\")\n",
    "print(f\"Validation size of positive reviews: {v_pos_cnt}\")\n",
    "print(f\"Positive samples ratio train to validation: {t_pos_cnt / v_pos_cnt}\\n\")\n",
    "\n",
    "print(f\"Training size of negative reviews: {t_neg_cnt}\")\n",
    "print(f\"Validation size of negative reviews: {v_neg_cnt}\")\n",
    "print(f\"Negative samples ratio train to validation: {t_pos_cnt / v_pos_cnt}\\n\")\n",
    "\n",
    "print(f\"Training size of neutral reviews: {t_neutr_cnt}\")\n",
    "print(f\"Validation size of neutral reviews: {v_neutr_cnt}\")\n",
    "print(f\"Neutral samples ratio train to validation: {t_pos_cnt / v_pos_cnt}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler = RandomSampler(train_dataset),\n",
    "    batch_size = BATCH_SIZE\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset,\n",
    "    sampler = SequentialSampler(validation_dataset),\n",
    "    batch_size = BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "\n",
    "BERT_PRETRAINED_MODEL = 'bert-base-uncased'\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    BERT_PRETRAINED_MODEL, \n",
    "    num_labels = 3, \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "Output layer\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (3, 768)\n",
      "classifier.bias                                                 (3,)\n"
     ]
    }
   ],
   "source": [
    "# Checking parameters\n",
    "# We will check the embedding layer and the output layer\n",
    "\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('Embedding layer')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\nOutput layer')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-5 # default value from args.learning_rate\n",
    "EPS_ERR = 1e-8 # default value from args.eps_err\n",
    "\n",
    "EPOCHS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr = LEARNING_RATE,\n",
    "    eps = EPS_ERR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy function\n",
    "def compute_accuracy(predictions, labels):\n",
    "    predictions_flattened = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flattened = labels.flatten()\n",
    "\n",
    "    return np.sum(predictions_flattened == labels_flattened) / len(labels_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EPOCH 0 / 4 ===\n",
      "\tBatch 25 of 50. Elapsed: 0:00:10.\n",
      "\t Average training loss: 0.8068425112962723\n",
      "\t Training epoch time: 0:00:17\n",
      "  Accuracy: 0.78\n",
      "  Validation Loss: 0.60\n",
      "  Validation took: 0:00:01\n",
      "=== EPOCH 1 / 4 ===\n",
      "\tBatch 25 of 50. Elapsed: 0:00:07.\n",
      "\t Average training loss: 0.41965128272771834\n",
      "\t Training epoch time: 0:00:15\n",
      "  Accuracy: 0.81\n",
      "  Validation Loss: 0.54\n",
      "  Validation took: 0:00:01\n",
      "=== EPOCH 2 / 4 ===\n",
      "\tBatch 25 of 50. Elapsed: 0:00:07.\n",
      "\t Average training loss: 0.2024732592701912\n",
      "\t Training epoch time: 0:00:14\n",
      "  Accuracy: 0.81\n",
      "  Validation Loss: 0.62\n",
      "  Validation took: 0:00:01\n",
      "=== EPOCH 3 / 4 ===\n",
      "\tBatch 25 of 50. Elapsed: 0:00:07.\n",
      "\t Average training loss: 0.10443506196141243\n",
      "\t Training epoch time: 0:00:15\n",
      "  Accuracy: 0.81\n",
      "  Validation Loss: 0.62\n",
      "  Validation took: 0:00:01\n",
      " === Training complete ===\n",
      "Total training took 0:01:06 (hh:mm:ss).\n"
     ]
    }
   ],
   "source": [
    "training_statistics = [] \n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    print(f\"=== EPOCH {epoch} / {EPOCHS} ===\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        if step % 25 == 0 and not step == 0:\n",
    "            elapsed = str(datetime.timedelta(seconds = int(round((time.time() - t0)))))\n",
    "\n",
    "            print(f\"\\tBatch {step} of {len(train_dataloader)}. Elapsed: {elapsed}.\")\n",
    "        \n",
    "        batch_input_ids = batch[0].to(device)\n",
    "        batch_input_mask = batch[1].to(device)\n",
    "        batch_labels = batch[2].to(device)\n",
    "\n",
    "        # Clear previously calculated gradients before the backward pass\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss, logits = model(\n",
    "            batch_input_ids, \n",
    "            token_type_ids = None, \n",
    "            attention_mask = batch_input_mask, \n",
    "            labels = batch_labels,\n",
    "            return_dict = False\n",
    "        )\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    training_time = str(datetime.timedelta(seconds = int(round((time.time() - t0)))))\n",
    "\n",
    "    print(f\"\\t Average training loss: {avg_train_loss}\")\n",
    "    print(f\"\\t Training epoch time: {training_time}\")\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch_input_ids = batch[0].to(device)\n",
    "        batch_input_mask = batch[1].to(device)\n",
    "        batch_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss, logits = model(\n",
    "                batch_input_ids, \n",
    "                token_type_ids = None, \n",
    "                attention_mask = batch_input_mask, \n",
    "                labels = batch_labels,\n",
    "                return_dict = False\n",
    "            )\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = batch_labels.cpu().numpy()\n",
    "\n",
    "        total_eval_accuracy += compute_accuracy(logits, label_ids)\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "    \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = str(datetime.timedelta(seconds = int(round((time.time() - t0)))))\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_statistics.append(\n",
    "        {\n",
    "            'epoch': epoch + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\" === Training complete ===\")\n",
    "\n",
    "print(f\"Total training took {str(datetime.timedelta(seconds= int(round((time.time() - total_t0)))))} (hh:mm:ss).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview training evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0:00:17</td>\n",
       "      <td>0:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0:00:15</td>\n",
       "      <td>0:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0:00:14</td>\n",
       "      <td>0:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0:00:15</td>\n",
       "      <td>0:00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.81         0.60           0.78       0:00:17         0:00:01\n",
       "2               0.42         0.54           0.81       0:00:15         0:00:01\n",
       "3               0.20         0.62           0.81       0:00:14         0:00:01\n",
       "4               0.10         0.62           0.81       0:00:15         0:00:01"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('precision', 2)\n",
    "\n",
    "df_statistics = pd.DataFrame(data = training_statistics)\n",
    "df_statistics = df_statistics.set_index('epoch')\n",
    "\n",
    "df_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving fine-tuned BERT to ./bert_fine_tuned\n"
     ]
    }
   ],
   "source": [
    "output_dir = './bert_fine_tuned'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(f\"Saving fine-tuned BERT to {output_dir}\")\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, 'model.pth'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
