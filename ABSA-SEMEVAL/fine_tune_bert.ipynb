{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semeval_reader import SemevalReader\n",
    "from utils import get_target_list_for_polarity, get_polarity_from_target_list, print_split_by_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelDataset import ModelDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_reader = SemevalReader('semeval16_restaurants_train.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = semeval_reader.read_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. reviews: 350\n",
      "No. sentences: 2000\n"
     ]
    }
   ],
   "source": [
    "print(f\"No. reviews: {len(reviews)}\")\n",
    "print(f\"No. sentences: {sum(map(lambda x: len(x.sentences), reviews))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_polarity_sentences = semeval_reader.get_absolute_polarity_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. sentences that have only one polarity: 1565\n"
     ]
    }
   ],
   "source": [
    "print(f\"No. sentences that have only one polarity: {len(absolute_polarity_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_polarity_df = pd.DataFrame(map(lambda x: (x.text, x.opinions[0].polarity), absolute_polarity_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_polarity_df.rename(columns={0: 'text'}, inplace=True)\n",
    "absolute_polarity_df['target_list'] = absolute_polarity_df.apply(lambda row: get_target_list_for_polarity(row[1]), axis=1)\n",
    "\n",
    "absolute_polarity_df = absolute_polarity_df.drop(columns=[1])\n",
    "\n",
    "absolute_polarity_df.to_csv('absolute_polarity_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>judging from previous posts this used to be a ...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>we, there were four of us, arrived at noon - t...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they never brought us complimentary noodles, i...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>after all that, they complained to me about th...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text target_list\n",
       "0  judging from previous posts this used to be a ...   [1, 0, 0]\n",
       "1  we, there were four of us, arrived at noon - t...   [1, 0, 0]\n",
       "2  they never brought us complimentary noodles, i...   [1, 0, 0]\n",
       "3  the food was lousy - too sweet or too salty an...   [1, 0, 0]\n",
       "4  after all that, they complained to me about th...   [1, 0, 0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absolute_polarity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_absolute_polarity_df = copy.deepcopy(absolute_polarity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_absolute_polarity_df['WORD_COUNT'] = report_absolute_polarity_df['text'].apply(lambda text: 0 if text == None else len(text.split(' ')))\n",
    "report_absolute_polarity_df['polarity'] = report_absolute_polarity_df['target_list'].apply(lambda tl: get_polarity_from_target_list(tl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3 artists>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(report_absolute_polarity_df['polarity'].value_counts().index, report_absolute_polarity_df['polarity'].value_counts().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    1040\n",
       "negative     473\n",
       "neutral       52\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_absolute_polarity_df['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Count')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(report_absolute_polarity_df['WORD_COUNT'].value_counts().values, report_absolute_polarity_df['WORD_COUNT'].value_counts().index)\n",
    "plt.xlabel(\"Length of text (words)\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-05\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (1565, 2)\n",
      "TRAIN Dataset: (1252, 2)\n",
      "TEST Dataset: (313, 2)\n"
     ]
    }
   ],
   "source": [
    "bert_train_size = 0.8\n",
    "bert_train_dataset=absolute_polarity_df.sample(frac=bert_train_size)\n",
    "bert_test_dataset=absolute_polarity_df.drop(bert_train_dataset.index).reset_index(drop=True)\n",
    "bert_train_dataset = bert_train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(absolute_polarity_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(bert_train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(bert_test_dataset.shape))\n",
    "\n",
    "bert_training_set = ModelDataset(bert_train_dataset, bert_tokenizer)\n",
    "bert_testing_set = ModelDataset(bert_test_dataset, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset\n",
      "\tPositive: 836\n",
      "\tNeutral: 46\n",
      "\tNegative: 370\n",
      "Test Dataset\n",
      "\tPositive: 204\n",
      "\tNeutral: 6\n",
      "\tNegative: 103\n"
     ]
    }
   ],
   "source": [
    "print_split_by_labels(bert_training_set, bert_testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_dataloader = DataLoader(\n",
    "    bert_training_set,\n",
    "    sampler = RandomSampler(bert_train_dataset),\n",
    "    batch_size = TRAIN_BATCH_SIZE\n",
    ")\n",
    "\n",
    "bert_validation_dataloader = DataLoader(\n",
    "    bert_testing_set,\n",
    "    sampler = SequentialSampler(bert_testing_set),\n",
    "    batch_size = VALID_BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "\n",
    "        self.out = torch.nn.Linear(768, 3)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.bert(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "\n",
    "        output_2 = self.dropout(output_1)\n",
    "        \n",
    "        output = self.out(output_2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (out): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = BERTClass()\n",
    "bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "bert_optimizer = torch.optim.Adam(params = bert_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for _,data in enumerate(bert_train_dataloader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        if _ % 500 == 0:\n",
    "            print(f'Epoch: {epoch}, Step: {_}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  0.7224812507629395\n",
      "Epoch: 1, Step: 0, Loss:  0.09074926376342773\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch, bert_model, loss_fn, bert_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(bert_validation_dataloader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            \n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.9009584664536742\n",
      "F1 Score (Micro) = 0.9009584664536742\n",
      "F1 Score (Macro) = 0.5970350404312669\n",
      "Precision Score (Micro) = 0.9009584664536742\n",
      "Precision Score (Macro) = 0.6010752688172043\n",
      "Recall Score (Micro) = 0.9009584664536742\n",
      "Recall Score (Macro) = 0.5953740719588806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "outputs, targets = validation(bert_model)\n",
    "outputs = np.argmax(outputs, axis=1)\n",
    "targets = np.argmax(targets, axis=1)\n",
    "accuracy = metrics.accuracy_score(targets, outputs)\n",
    "precision_score_micro = metrics.precision_score(targets, outputs, average='micro')\n",
    "precision_score_macro = metrics.precision_score(targets, outputs, average='macro')\n",
    "recall_score_micro = metrics.recall_score(targets, outputs, average='micro')\n",
    "recall_score_macro = metrics.recall_score(targets, outputs, average='macro')\n",
    "f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "print(f\"Accuracy Score = {accuracy}\")\n",
    "print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
    "print(f\"Precision Score (Micro) = {precision_score_micro}\")\n",
    "print(f\"Precision Score (Macro) = {precision_score_macro}\")\n",
    "print(f\"Recall Score (Micro) = {recall_score_micro}\")\n",
    "print(f\"Recall Score (Macro) = {recall_score_macro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bert_model.bert, 'bert_fine_tuned.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bert_model\n",
    "\n",
    "del loss_fn\n",
    "del bert_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_fine_tune_run = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Train Dataset\n",
      "\tPositive: 840\n",
      "\tNeutral: 39\n",
      "\tNegative: 373\n",
      "Test Dataset\n",
      "\tPositive: 200\n",
      "\tNeutral: 13\n",
      "\tNegative: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  0.7116765975952148\n",
      "Epoch: 1, Step: 0, Loss:  0.09428238868713379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2/10\n",
      "Train Dataset\n",
      "\tPositive: 833\n",
      "\tNeutral: 40\n",
      "\tNegative: 379\n",
      "Test Dataset\n",
      "\tPositive: 207\n",
      "\tNeutral: 12\n",
      "\tNegative: 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  0.6823946833610535\n",
      "Epoch: 1, Step: 0, Loss:  0.13427622616291046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3/10\n",
      "Train Dataset\n",
      "\tPositive: 822\n",
      "\tNeutral: 43\n",
      "\tNegative: 387\n",
      "Test Dataset\n",
      "\tPositive: 218\n",
      "\tNeutral: 9\n",
      "\tNegative: 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  0.6463196277618408\n",
      "Epoch: 1, Step: 0, Loss:  0.13950341939926147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4/10\n",
      "Train Dataset\n",
      "\tPositive: 833\n",
      "\tNeutral: 40\n",
      "\tNegative: 379\n",
      "Test Dataset\n",
      "\tPositive: 207\n",
      "\tNeutral: 12\n",
      "\tNegative: 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  0.75067538022995\n",
      "Epoch: 1, Step: 0, Loss:  0.1553804576396942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 5/10\n",
      "Train Dataset\n",
      "\tPositive: 825\n",
      "\tNeutral: 42\n",
      "\tNegative: 385\n",
      "Test Dataset\n",
      "\tPositive: 215\n",
      "\tNeutral: 10\n",
      "\tNegative: 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  0.7162754535675049\n",
      "Epoch: 1, Step: 0, Loss:  0.08361220359802246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6/10\n",
      "Train Dataset\n",
      "\tPositive: 843\n",
      "\tNeutral: 41\n",
      "\tNegative: 368\n",
      "Test Dataset\n",
      "\tPositive: 197\n",
      "\tNeutral: 11\n",
      "\tNegative: 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  0.8092023730278015\n",
      "Epoch: 1, Step: 0, Loss:  0.09687098115682602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 7/10\n",
      "Train Dataset\n",
      "\tPositive: 835\n",
      "\tNeutral: 38\n",
      "\tNegative: 379\n",
      "Test Dataset\n",
      "\tPositive: 205\n",
      "\tNeutral: 14\n",
      "\tNegative: 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  0.8028361797332764\n",
      "Epoch: 1, Step: 0, Loss:  0.10998150706291199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 8/10\n",
      "Train Dataset\n",
      "\tPositive: 834\n",
      "\tNeutral: 49\n",
      "\tNegative: 369\n",
      "Test Dataset\n",
      "\tPositive: 206\n",
      "\tNeutral: 3\n",
      "\tNegative: 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  0.6743930578231812\n",
      "Epoch: 1, Step: 0, Loss:  0.382513165473938\n",
      "Run 9/10\n",
      "Train Dataset\n",
      "\tPositive: 823\n",
      "\tNeutral: 40\n",
      "\tNegative: 389\n",
      "Test Dataset\n",
      "\tPositive: 217\n",
      "\tNeutral: 12\n",
      "\tNegative: 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  0.7102643251419067\n",
      "Epoch: 1, Step: 0, Loss:  0.5295178890228271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 10/10\n",
      "Train Dataset\n",
      "\tPositive: 834\n",
      "\tNeutral: 37\n",
      "\tNegative: 381\n",
      "Test Dataset\n",
      "\tPositive: 206\n",
      "\tNeutral: 15\n",
      "\tNegative: 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  0.7807523608207703\n",
      "Epoch: 1, Step: 0, Loss:  0.26417455077171326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/10\")\n",
    "\n",
    "    bert_train_size = 0.8\n",
    "    bert_train_dataset=absolute_polarity_df.sample(frac=bert_train_size)\n",
    "    bert_test_dataset=absolute_polarity_df.drop(bert_train_dataset.index).reset_index(drop=True)\n",
    "    bert_train_dataset = bert_train_dataset.reset_index(drop=True)\n",
    "\n",
    "    bert_training_set = ModelDataset(bert_train_dataset, bert_tokenizer)\n",
    "    bert_testing_set = ModelDataset(bert_test_dataset, bert_tokenizer)\n",
    "\n",
    "    print_split_by_labels(bert_training_set, bert_testing_set)\n",
    "\n",
    "    bert_train_dataloader = DataLoader(\n",
    "        bert_training_set,\n",
    "        sampler = RandomSampler(bert_train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    bert_validation_dataloader = DataLoader(\n",
    "        bert_testing_set,\n",
    "        sampler = SequentialSampler(bert_testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    bert_model_run = BERTClass()\n",
    "    bert_model_run.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = bert_model_run.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, bert_model_run, loss_fn, optimizer)\n",
    "\n",
    "    outputs, targets = validation(bert_model_run)\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "    targets = np.argmax(targets, axis=1)\n",
    "    accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    precision_score_micro = metrics.precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = metrics.precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = metrics.recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = metrics.recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    bert_fine_tune_run.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    del bert_train_dataset\n",
    "    del bert_test_dataset\n",
    "    del bert_training_set\n",
    "    del bert_testing_set\n",
    "    del bert_model_run\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.907348</td>\n",
       "      <td>0.907348</td>\n",
       "      <td>0.596835</td>\n",
       "      <td>0.907348</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.907348</td>\n",
       "      <td>0.612722</td>\n",
       "      <td>186.395960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.907348</td>\n",
       "      <td>0.907348</td>\n",
       "      <td>0.594757</td>\n",
       "      <td>0.907348</td>\n",
       "      <td>0.621869</td>\n",
       "      <td>0.907348</td>\n",
       "      <td>0.607969</td>\n",
       "      <td>185.522013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.872204</td>\n",
       "      <td>0.872204</td>\n",
       "      <td>0.605499</td>\n",
       "      <td>0.872204</td>\n",
       "      <td>0.548859</td>\n",
       "      <td>0.872204</td>\n",
       "      <td>0.565108</td>\n",
       "      <td>184.061373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.888179</td>\n",
       "      <td>0.888179</td>\n",
       "      <td>0.577564</td>\n",
       "      <td>0.888179</td>\n",
       "      <td>0.610272</td>\n",
       "      <td>0.888179</td>\n",
       "      <td>0.593054</td>\n",
       "      <td>184.818004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.881789</td>\n",
       "      <td>0.881789</td>\n",
       "      <td>0.569228</td>\n",
       "      <td>0.881789</td>\n",
       "      <td>0.600194</td>\n",
       "      <td>0.881789</td>\n",
       "      <td>0.583756</td>\n",
       "      <td>185.216141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.865815</td>\n",
       "      <td>0.865815</td>\n",
       "      <td>0.576437</td>\n",
       "      <td>0.865815</td>\n",
       "      <td>0.587527</td>\n",
       "      <td>0.865815</td>\n",
       "      <td>0.581472</td>\n",
       "      <td>184.536646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.884984</td>\n",
       "      <td>0.884984</td>\n",
       "      <td>0.576391</td>\n",
       "      <td>0.884984</td>\n",
       "      <td>0.623214</td>\n",
       "      <td>0.884984</td>\n",
       "      <td>0.597042</td>\n",
       "      <td>185.456679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.926518</td>\n",
       "      <td>0.926518</td>\n",
       "      <td>0.627030</td>\n",
       "      <td>0.926518</td>\n",
       "      <td>0.605738</td>\n",
       "      <td>0.926518</td>\n",
       "      <td>0.614080</td>\n",
       "      <td>184.582411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.859425</td>\n",
       "      <td>0.859425</td>\n",
       "      <td>0.557392</td>\n",
       "      <td>0.859425</td>\n",
       "      <td>0.573733</td>\n",
       "      <td>0.859425</td>\n",
       "      <td>0.565343</td>\n",
       "      <td>184.570498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.881789</td>\n",
       "      <td>0.881789</td>\n",
       "      <td>0.570969</td>\n",
       "      <td>0.881789</td>\n",
       "      <td>0.621043</td>\n",
       "      <td>0.881789</td>\n",
       "      <td>0.592665</td>\n",
       "      <td>185.581295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.907348               0.907348               0.596835            0.907348   \n",
       "1  0.907348               0.907348               0.594757            0.907348   \n",
       "2  0.872204               0.872204               0.605499            0.872204   \n",
       "3  0.888179               0.888179               0.577564            0.888179   \n",
       "4  0.881789               0.881789               0.569228            0.881789   \n",
       "5  0.865815               0.865815               0.576437            0.865815   \n",
       "6  0.884984               0.884984               0.576391            0.884984   \n",
       "7  0.926518               0.926518               0.627030            0.926518   \n",
       "8  0.859425               0.859425               0.557392            0.859425   \n",
       "9  0.881789               0.881789               0.570969            0.881789   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.630000        0.907348        0.612722      186.395960  \n",
       "1            0.621869        0.907348        0.607969      185.522013  \n",
       "2            0.548859        0.872204        0.565108      184.061373  \n",
       "3            0.610272        0.888179        0.593054      184.818004  \n",
       "4            0.600194        0.881789        0.583756      185.216141  \n",
       "5            0.587527        0.865815        0.581472      184.536646  \n",
       "6            0.623214        0.884984        0.597042      185.456679  \n",
       "7            0.605738        0.926518        0.614080      184.582411  \n",
       "8            0.573733        0.859425        0.565343      184.570498  \n",
       "9            0.621043        0.881789        0.592665      185.581295  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_fine_tune_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_fine_tune_run.to_csv(\"bert_fine_tune_10runs.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
